{
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom string import punctuation\ncustom = stop_words+list(punctuation)\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nnltk.download('wordnet')\ndef my_tokenizer(s):\n    try:\n        s = s.lower() # downcase\n    except:\n        s = str(s).lower()\n    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n    tokens = [t for t in tokens if t not in custom] # remove stopwords\n    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove any digits, i.e. \"3rd edition\"\n    return tokens\n\ndef clean_text(df, column_1, column_2):\n    df = df[[column_1, column_2]]\n    text = df[column_1].tolist()\n    cleaned_text = []\n    for x in text:\n        cleaned_text.append(my_tokenizer(x))\n    joined_text = []\n    for x in cleaned_text:\n        joined_text.append(' '.join(x))\n    df1 = pd.DataFrame({'Message':joined_text})\n    df1[\"sentiment\"] = pd.Series(df[column_2].tolist())\n    df1[\"sentiment\"] = df1[\"sentiment\"].str.lower()\n    return df1",
      "metadata": {
        "trusted": true
      },
      "execution_count": 106,
      "outputs": [
        {
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df_jan = pd.read_excel(\"output_jan.xlsx\")\ndf_feb = pd.read_excel(\"output_feb.xlsx\")\ndf_mar = pd.read_excel(\"output_mar.xlsx\")\ndf_apr = pd.read_csv(\"output_apr.txt\")\ndf_may = pd.read_csv(\"output_may.txt\")\ndf_jun = pd.read_csv(\"output_june.txt\")\ndf_jan_cleaned = clean_text(df_jan, \"Message\", \"Revsied\")\ndf_feb_cleaned = clean_text(df_feb, \"Message\", \"revised\")\ndf_mar_cleaned = clean_text(df_mar, \"Message\", \"Revised\")\ndf_apr_cleaned = clean_text(df_apr, \"Message\", \"revised\")\ndf_may_cleaned = clean_text(df_may, \"Message\", \"Revised\")\ndf_jun_cleaned = clean_text(df_jun, \"Message\", \"Revised\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words=\"english\")\nle = LabelEncoder()\nfrom sklearn.metrics import accuracy_score,roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.naive_bayes import MultinomialNB\n!pip install imblearn\nfrom imblearn.over_sampling import SMOTE\ndef vectorize_and_encode(df):\n    df = df.dropna()\n    tfidf.fit(df[\"Message\"])\n    vector = tfidf.transform(df[\"Message\"])\n    vector_values_array = vector.toarray()\n    X = vector_values_array\n    y = le.fit_transform(df[\"sentiment\"])\n    return X, y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 108,
      "outputs": [
        {
          "text": "Requirement already satisfied: imblearn in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.0)\nRequirement already satisfied: imbalanced-learn in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from imblearn) (0.4.3)\nRequirement already satisfied: scipy>=0.13.3 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.1.0)\nRequirement already satisfied: numpy>=1.8.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.14.6)\nRequirement already satisfied: scikit-learn>=0.20 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.20.3)\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df = pd.concat([df_jan_cleaned, df_feb_cleaned, df_mar_cleaned, df_apr_cleaned, df_jun_cleaned])",
      "metadata": {
        "trusted": true
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# X,y = vectorize_and_encode(df)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# X.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "df.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 112,
      "outputs": [
        {
          "execution_count": 112,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(7021, 2)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# def smote(X,y):\n#     sm = SMOTE(random_state=42)\n#     X_res, y_res = sm.fit_sample(X, y.ravel())\n#     return X,y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# X,y = smote(X,y)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# X.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import WordPunctTokenizer\ntok = WordPunctTokenizer()\npat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?://[A-Za-z0-9./]+'\ncombined_pat = r'|'.join((pat1, pat2))\ndef tweet_cleaner(text):\n    text = str(text)\n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    stripped = re.sub(combined_pat, '', souped)\n    try:\n        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        clean = stripped\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n    lower_case = letters_only.lower()\n    # During the letters_only process two lines above, it has created unnecessay white spaces,\n    # I will tokenize and join together to remove unneccessary white spaces\n    words = tok.tokenize(lower_case)\n    return (\" \".join(words)).strip()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def test(csv_file):\n    try:\n        test = pd.read_csv(csv_file)\n    except:\n        test = pd.read_csv(csv_file, encoding=\"Latin-1\")\n    message_work_msg_cleaned =[]\n    for x in range(test.shape[0]):\n        message_work_msg_cleaned.append(tweet_cleaner(test[\"content\"].iloc[x]))    \n    message_work_sentiment = test[\"sentiment\"].tolist()\n    df1 = pd.DataFrame({'Message':message_work_msg_cleaned})\n    df1[\"sentiment\"] = pd.Series(message_work_sentiment)\n    df1[\"sentiment\"] = df1[\"sentiment\"].str.lower()\n    return df1\n    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# message_work_msg_cleaned =[]\n# for x in range(test.shape[0]):\n#     message_work_msg_cleaned.append(tweet_cleaner(test[\"content\"].iloc[x]))    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# len(message_work_msg_cleaned)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# message_work_sentiment = test[\"sentiment\"].tolist()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# test = test(\"unilever ml.csv\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# test.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# test_cleaned = clean_text(test, \"Message\", \"sentiment\")\ntest_cleaned = df_may_cleaned.copy()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test_cleaned.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 124,
      "outputs": [
        {
          "execution_count": 124,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1923, 2)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 125,
      "outputs": [
        {
          "execution_count": 125,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(7021, 2)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# test_cleaned = df_may_cleaned.copy()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test_cleaned.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 127,
      "outputs": [
        {
          "execution_count": 127,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1923, 2)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "test_cleaned = test_cleaned.dropna()\ntest_cleaned.isnull().sum()\ndf = df.dropna()\ndf.isnull().sum()\nprint (test_cleaned.shape)\nprint (df.shape)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 128,
      "outputs": [
        {
          "text": "(1923, 2)\n(6819, 2)\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "8742+483\n# 1923+6819\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 129,
      "outputs": [
        {
          "execution_count": 129,
          "output_type": "execute_result",
          "data": {
            "text/plain": "9225"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "def vectorize_test(test):\n    df1 = pd.concat([df, test])\n    tfidf.fit(df1[\"Message\"])\n    vector = tfidf.transform(df1[\"Message\"])\n    vector_values_array = vector.toarray()\n    X = vector_values_array\n    y = le.fit_transform(df[\"sentiment\"])\n    return X, y",
      "metadata": {
        "trusted": true
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X,y = vectorize_test(test_cleaned)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 132,
      "outputs": [
        {
          "execution_count": 132,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(8742, 8705)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "y.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 133,
      "outputs": [
        {
          "execution_count": 133,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(6819,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "X_train = X[:y.shape[0]]\ny_train = y\nX_test = X[y.shape[0]:]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y_test = le.fit_transform(test_cleaned[\"sentiment\"])",
      "metadata": {
        "trusted": true
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y_test.shape",
      "metadata": {
        "trusted": true
      },
      "execution_count": 136,
      "outputs": [
        {
          "execution_count": 136,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(1923,)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# sm = SMOTE(random_state=42)\n# X_train, y_train = sm.fit_sample(X_train, y_train.ravel())\n\n# nb = MultinomialNB()\n\n# nb.fit(X_train,y_train)\n# y_pred = nb.predict(X_test)\n# print (accuracy_score(y_test,y_pred))\n# print (classification_report(y_test,y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nsm = SMOTE(random_state=42)\nX_train, y_train = sm.fit_sample(X_train, y_train.ravel())\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\nprint (accuracy_score(y_test,y_pred))\nprint (classification_report(y_test,y_pred))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 137,
      "outputs": [
        {
          "text": "0.7914716588663546\n              precision    recall  f1-score   support\n\n           0       0.58      0.66      0.62       312\n           1       0.85      0.86      0.85      1243\n           2       0.81      0.68      0.74       368\n\n   micro avg       0.79      0.79      0.79      1923\n   macro avg       0.75      0.73      0.74      1923\nweighted avg       0.80      0.79      0.79      1923\n\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}